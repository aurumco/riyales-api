# Workflow name
name: Market Data Sync

# Triggers for the workflow
on:
  # Runs every 5 minutes
  schedule:
    - cron: '*/5 * * * *'
  
  # Allows manual triggering from the GitHub Actions UI
  workflow_dispatch:
    inputs:
      upload_v1_data:
        description: 'Upload all v1 directories (true/false)'
        required: true
        default: false
        type: boolean
        
  # Runs on pushes to the main branch
  push:
    branches:
      - main

# Environment variables available to all jobs
env:
  PYTHON_VERSION: '3.13'

# Defines the jobs to be run
jobs:
  # The main job for syncing data
  sync-data:
    name: Sync
    runs-on: ubuntu-latest
    permissions:
      # Required to commit and push changes back to the repository
      contents: write
      
    env:
      BRS_API_KEY: ${{ secrets.BRS_API_KEY }}
      BRS_BASE_URL: ${{ secrets.BRS_BASE_URL }}
      TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
      TELEGRAM_USER_ID: ${{ secrets.TELEGRAM_USER_ID }}
      LOG_LEVELS: ${{ secrets.LOG_LEVELS || 'ERROR' }}
      
    steps:
      # Step 1: Check out the repository code
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          # Fetch all history for all branches and tags
          fetch-depth: 0

      # Step 2: Set up the specified Python version
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      # Step 3: Cache pip dependencies for faster installs
      - name: Restore pip cache
        uses: actions/cache@v4
        with:
          # The directory to cache
          path: ~/.cache/pip
          # A unique key for the cache, based on OS and requirements files
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
          # Fallback keys if the primary key is not found
          restore-keys: |
            ${{ runner.os }}-pip-

      # NOTE: The problematic 'apt' cache step has been removed.
      # Caching apt packages for a single small tool like 'lftp' adds complexity
      # (due to system file permissions) for negligible performance gain on fast GitHub runners.
      # It's simpler and more reliable to install it directly each time.

      # Step 4: Install Python dependencies
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
        shell: bash

      # Step 5: Run the main data fetching script
      - name: Run data-fetch script
        run: |
          mkdir -p logs
          python src/main.py
        shell: bash

      # Step 6: Pull latest changes, commit, and push any new data
      - name: Pull latest & Commit if any
        run: |
          git config --global user.name "Market Data Bot"
          git config --global user.email "market-data-bot@users.noreply.github.com"
          
          echo "• Pulling latest changes from origin/main"
          # Use rebase with autostash to handle local changes cleanly
          if ! git pull --rebase --autostash origin main; then
            echo "• ERROR: Git pull failed with conflicts or errors. Aborting rebase and trying a standard pull."
            git rebase --abort 2>/dev/null || true
            git pull origin main || echo "• ERROR: Fallback git pull also failed."
          fi
          
          echo "• Checking for changes in market data files and logs..."
          git add api/v1/
          git add api/v2/market/
          git add logs/
          
          # Find and unstage files larger than 100MB to avoid Git LFS issues
          find api/v1/ api/v2/market/ -type f -size +100M | while read file; do
            echo "• Skipping large file from commit: $file"
            git restore --staged "$file"
          done
          
          # Commit only if there are staged changes
          if ! git diff --staged --quiet; then
            echo "• Changes detected in market data files. Committing..."
            MSG=$(python src/commit_message.py)
            git commit -m "$MSG" -m "Triggered by: ${{ github.event_name }}"
            
            echo "• Pushing changes to origin/main"
            if ! git push origin main; then
              echo "• ERROR: Git push failed."
            fi
          else
            echo "• No changes to market data files. Skipping commit."
          fi
        shell: bash

      # Step 7: Install lftp and upload data to the FTP server
      - name: Upload API data to Download Server via FTP
        run: |
          echo "• Installing lftp client..."
          # Update package lists and install lftp. Using -yq for quiet non-interactive install.
          sudo apt-get update -yq && sudo apt-get install -yq lftp
          
          # Define v1 directories for conditional upload
          V1_DIRECTORIES="api/v1/config api/v1/banner api/v1/ads api/v1/assets api/v1/terms"
          
          # Start building the lftp command array for better readability
          lftp_commands=(
            "open -u ${{ secrets.FTP_USERNAME }},${{ secrets.FTP_PASSWORD }} ftp://3117204815.cloudydl.com"
            "set ssl:verify-certificate no"
            "mkdir -p public_html/riyales/api/v2"
            "mkdir -p public_html/riyales/api/v1"
          )
          
          # Always sync v2 data
          echo "• Scheduling v2 directory for unconditional upload."
          lftp_commands+=("mirror -R --verbose --delete --parallel=5 api/v2/market/ public_html/riyales/api/v2/market")

          # Conditionally sync v1 directories ONLY on manual 'workflow_dispatch' trigger
          if [[ "${{ github.event_name }}" == "workflow_dispatch" && "${{ github.event.inputs.upload_v1_data }}" == "true" ]]; then
            echo "• Manual trigger active: Scheduling all v1 directories for upload."
            for dir_path in $V1_DIRECTORIES; do
              lftp_commands+=("mirror -R --verbose --delete --parallel=5 '$dir_path' 'public_html/riyales/api/v1/'")
            done
          else
            echo "• Skipping v1 directories upload. Manual trigger not activated."
          fi

          lftp_commands+=("bye")
          
          # Join the array of commands into a single string for lftp
          lftp_script=$(printf "%s;" "${lftp_commands[@]}")
          
          echo "--- Executing FTP Sync Script ---"
          lftp -c "$lftp_script"
          echo "--- FTP Sync Script Finished ---"
        shell: bash

      # NOTE: The 'Fix apt cache permissions' step has been removed as it's no longer necessary
      # after removing the apt cache step itself.

      # Step 8: Check for errors and send alerts if needed
      - name: Check and send error logs
        # This step runs even if previous steps have failed
        if: always()
        run: |
          ERROR_LOG="${GITHUB_WORKSPACE}/logs/error.log"
          echo "• Final check for error log at: ${ERROR_LOG}"
          
          if [ -f "${ERROR_LOG}" ]; then
            # Count lines that are not empty or just comments
            RELEVANT_LINES=$(grep -cvE '(^\s*$)|(^# Log cleared on)|(^# Log forcibly cleared on)' "${ERROR_LOG}" || true)
            
            if [ "${RELEVANT_LINES}" -gt 0 ]; then
              echo "• Error log has relevant content (${RELEVANT_LINES} lines). Running alert_sender.py..."
              cd "${GITHUB_WORKSPACE}"
              python src/alert_sender.py
              
              # As a safeguard, ensure the log is cleared after sending
              if [ -s "${ERROR_LOG}" ]; then
                echo "• WARNING: error.log was NOT truncated by the script. Forcing truncation." | tee -a "${ERROR_LOG}"
                echo "# Log forcibly cleared by workflow at $(date -u '+Y-%m-%dT%H:%M:%SZ')" > "${ERROR_LOG}"
              else
                echo "• error.log successfully truncated."
              fi
            else
              echo "• Error log is empty or contains only clear messages. Skipping alert."
            fi
          else
            echo "• No error.log file found. Skipping alerts."
          fi
        shell: bash

  # A separate job for cleaning up old workflow runs
  cleanup:
    name: Cleanup
    # This job depends on the completion of 'sync-data'
    needs: sync-data
    # It runs regardless of whether 'sync-data' succeeded or failed
    if: always()
    runs-on: ubuntu-latest
    permissions:
      # Required to delete workflow runs
      actions: write
    env:
      GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      REPO: ${{ github.repository }}
      WF: ${{ github.workflow }}
    steps:
      - name: Remove old workflow runs
        run: |
          # List all runs, filter out in-progress ones, sort by date, and get IDs to delete (keeping the latest 9)
          runs=$(gh run list \
            --repo "$REPO" \
            --workflow "$WF" \
            --limit 100 \
            --json databaseId,status,createdAt \
            --jq '.[] | select(.status!="in_progress" and .status!="queued") | {id: .databaseId, date: .createdAt}' | \
            jq -s 'sort_by(.date) | reverse | .[].id')
            
          total=$(echo "$runs" | wc -l)
          if [ "$total" -le 9 ]; then
            echo "• Found $total runs, no cleanup needed (keeps latest 9)."
            exit 0
          fi
          
          to_delete=$(echo "$runs" | tail -n +10)
          echo "• Deleting oldest workflow runs..."
          echo "$to_delete" | while read id; do
            echo "  - Deleting run ID: $id"
            gh run delete "$id" --repo "$REPO"
          done
        shell: bash
