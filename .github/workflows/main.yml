name: Market Data Sync

on:
  schedule:
    - cron: '*/5 * * * *'
  workflow_dispatch:
  push:
    branches:
      - main

env:
  PYTHON_VERSION: '3.13'

jobs:
  sync-data:
    name: Sync
    runs-on: ubuntu-latest
    permissions:
      contents: write
    env:
      BRS_API_KEY: ${{ secrets.BRS_API_KEY }}
      BRS_BASE_URL: ${{ secrets.BRS_BASE_URL }}
      TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
      TELEGRAM_USER_ID: ${{ secrets.TELEGRAM_USER_ID }}
      LOG_LEVELS: ${{ secrets.LOG_LEVELS || 'ERROR' }}
    steps:

      - name: Checkout code
        uses: actions/checkout@v4
        with:
          # Fetch last 2 commits to be able to compare HEAD with the previous one
          fetch-depth: 2

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Restore pip cache
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      # --- Caching for apt packages like lftp ---
      # This step caches the downloaded .deb file for lftp.
      # While `apt-get update` still runs, `apt-get install` will be much faster
      # as it will use the cached package instead of downloading it again.
      - name: Cache apt packages for lftp
        uses: actions/cache@v4
        with:
          path: /var/cache/apt/archives
          key: ${{ runner.os }}-apt-lftp-cache-${{ hashFiles('**/requirements*.txt') }} # Added a file hash to the key
          restore-keys: |
            ${{ runner.os }}-apt-lftp-cache-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
        shell: bash

      - name: Run data-fetch script
        run: |
          mkdir -p logs
          python src/main.py
        shell: bash

      - name: Pull latest & Commit if any
        run: |
          git config --global user.name "Market Data Bot"
          git config --global user.email "market-data-bot@users.noreply.github.com"
          
          echo "• Pulling latest changes from origin/main"
          if ! git pull --rebase --autostash origin main; then
            echo "• ERROR: Git pull failed with conflicts or errors"
            git rebase --abort 2>/dev/null || true
            git pull origin main || echo "• ERROR: Fallback git pull also failed"
          fi
          
          echo "• Checking for changes in market data files and logs..."
          git add api/v1/
          git add api/v2/market/
          git add logs/
          
          find api/v1/ api/v2/market/ -type f -size +100M | while read file; do
            echo "• Skipping large file from commit: $file"
            git restore --staged "$file"
          done
          
          if ! git diff --staged --quiet; then
            echo "• Changes detected in market data files. Committing..."
            MSG=$(python src/commit_message.py)
            git commit -m "$MSG" -m "Triggered by: ${{ github.event_name }}"
            
            echo "• Pushing changes to origin/main"
            if ! git push origin main; then
              echo "• ERROR: Git push failed"
            fi
          else
            echo "• No changes to market data files. Skipping commit."
          fi
        shell: bash

      # --- CORRECTED: Conditional FTP upload based on actual file content changes ---
      - name: Upload API data to Download Server via FTP
        run: |
          # Install lftp client (will use cache if available)
          sudo apt-get update && sudo apt-get -y install lftp

          # Define directories to check for changes
          DIRECTORIES_TO_CHECK="api/v1/config api/v1/banner api/v1/ads api/v1/assets api/v1/terms"
          
          # Get the commit hash of the parent commit. We need this to compare against.
          # This is why `fetch-depth: 2` is important in the checkout step.
          PREVIOUS_COMMIT=$(git rev-parse HEAD~1)
          
          # A flag to track if any directory has changed
          HAS_CHANGES=false

          # Start building the lftp command array
          lftp_commands=(
            "open -u ${{ secrets.FTP_USERNAME }},${{ secrets.FTP_PASSWORD }} ftp://3117204815.cloudydl.com"
            "set ssl:verify-certificate no"
            # Part 1: Always sync critical v2 data (Protobuf)
            "mkdir -p public_html/riyales/api/v2"
            "cd public_html/riyales/api/v2"
            # CORRECTED PATH: Removed incorrect relative path prefix
            "mirror -R --verbose --delete --parallel=5 api/v2/market/ market"
            # Part 2: Navigate to v1 for conditional syncs
            "cd ../v1"
          )

          # Loop through each directory and check for actual content changes using git diff
          for dir_path in $DIRECTORIES_TO_CHECK; do
            # `git diff --quiet` exits with 1 if there are changes, and 0 if not.
            # We check if the command fails (i.e., there are changes).
            if ! git diff --quiet $PREVIOUS_COMMIT HEAD -- "$dir_path"; then
              echo "• Actual content changes detected in '$dir_path'. Scheduling for upload."
              HAS_CHANGES=true
              remote_dir_name=$(basename "$dir_path")
              # CORRECTED PATH: Removed incorrect relative path prefix
              lftp_commands+=("mirror -R --verbose --delete --parallel=5 '$dir_path' '$remote_dir_name'")
            else
              echo "• No actual content changes in '$dir_path'. Skipping upload."
            fi
          done

          # Only execute FTP commands if there were actual changes in the checked directories
          # or if we are always syncing the v2 data (which we are).
          # The check `if [ "$HAS_CHANGES" = true ]` can be added if you want to skip FTP entirely
          # when only v2 changes, but the current logic always syncs v2 and conditionally syncs v1 dirs.

          # Add the final 'bye' command to close the connection
          lftp_commands+=("bye")

          # Join the array of commands into a single script string
          lftp_script=$(printf "%s;" "${lftp_commands[@]}")

          # Execute the dynamically generated script
          echo "--- Executing FTP Sync Script ---"
          lftp -c "$lftp_script"
          echo "--- FTP Sync Script Finished ---"
        shell: bash

      - name: Check and send error logs
        if: always()
        run: |
          ERROR_LOG="${GITHUB_WORKSPACE}/logs/error.log"
          echo "• Final check for error log at: ${ERROR_LOG}"
          
          if [ -f "${ERROR_LOG}" ]; then
            RELEVANT_LINES=$(grep -cvE '(^\s*$)|(^# Log cleared on)|(^# Log forcibly cleared on)' "${ERROR_LOG}" || true)
            
            if [ "${RELEVANT_LINES}" -gt 0 ]; then
              echo "• Error log has relevant content (${RELEVANT_LINES} lines). Running alert_sender.py..."
              cd "${GITHUB_WORKSPACE}"
              python src/alert_sender.py
              
              if [ -s "${ERROR_LOG}" ]; then
                echo "• WARNING: error.log was NOT truncated. Forcing truncation." | tee -a "${ERROR_LOG}"
                echo "# Log forcibly cleared by workflow at $(date -u '+%Y-%m-%dT%H:%M:%SZ')" > "${ERROR_LOG}"
              else
                echo "• error.log successfully truncated."
              fi
            else
              echo "• Error log is empty or contains only clear messages. Skipping alert."
            fi
          else
            echo "• No error.log file found. Skipping alerts."
          fi
        shell: bash

  cleanup:
    name: Cleanup
    needs: sync-data
    if: always()
    runs-on: ubuntu-latest
    permissions:
      actions: write
    env:
      GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      REPO: ${{ github.repository }}
      WF: ${{ github.workflow }}
    steps:
      - name: Remove old workflow runs
        run: |
          runs=$(gh run list \
            --repo "$REPO" \
            --workflow "$WF" \
            --limit 100 \
            --json databaseId,status,createdAt \
            --jq '.[] | select(.status!="in_progress" and .status!="queued") | {id: .databaseId, date: .createdAt}' | \
            jq -s 'sort_by(.date) | reverse | .[].id')
          total=$(echo "$runs" | wc -l)
          if [ "$total" -le 9 ]; then exit 0; fi
          to_delete=$(echo "$runs" | tail -n +10)
          echo "$to_delete" | while read id; do
            echo "Deleting run ID: $id"
            gh run delete "$id" --repo "$REPO"
          done
        shell: bash
