name: Market Data Sync

on:
  schedule:
    - cron: '*/5 * * * *'
  workflow_dispatch:
  push:
    branches:
      - main

env:
  PYTHON_VERSION: '3.13'

jobs:
  sync-data:
    name: Sync
    runs-on: ubuntu-latest
    permissions:
      contents: write
    env:
      BRS_API_KEY:        ${{ secrets.BRS_API_KEY }}
      BRS_BASE_URL:       ${{ secrets.BRS_BASE_URL }}
      TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
      TELEGRAM_USER_ID:   ${{ secrets.TELEGRAM_USER_ID }}
      LOG_LEVELS:         ${{ secrets.LOG_LEVELS || 'ERROR' }}
    steps:

      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Restore pip cache
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Cache apt packages
        uses: actions/cache@v4
        with:
          path: |
            /var/cache/apt/archives
            /var/lib/apt/lists
          key: ${{ runner.os }}-apt-packages-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-apt-packages-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
        shell: bash

      - name: Run data-fetch script
        run: |
          # Ensure logs directory exists for app.log/error.log
          mkdir -p logs
          # Execute main script (logging handled internally)
          python src/main.py
        shell: bash

      - name: Pull latest & Commit if any
        run: |
          # Setup git
          git config --global user.name "Market Data Bot"
          git config --global user.email "market-data-bot@users.noreply.github.com"
          
          # Try to pull latest changes safely
          echo "• Pulling latest changes from origin/main"
          if ! git pull --rebase --autostash origin main; then
            echo "• ERROR: Git pull failed with conflicts or errors"
            git rebase --abort 2>/dev/null || true
            git pull origin main || echo "• ERROR: Fallback git pull also failed"
          fi
          
          # Commit market data changes and log files
          echo "• Checking for changes in market data files and logs..."
          git add api/v1/config/
          git add api/v2/market/
          git add logs/
          
          # Unstage any files larger than 100MB to avoid commit failures
          find api/v1/config/ api/v2/market/ -type f -size +100M | while read file; do
            echo "• Skipping large file from commit: $file"
            git restore --staged "$file"
          done
          
          # Commit only if there are actual changes to market data files
          if ! git diff --staged --quiet; then
            echo "• Changes detected in market data files. Committing..."
            MSG=$(python src/commit_message.py)
            git commit -m "$MSG" -m "Triggered by: ${{ github.event_name }}"
            
            echo "• Pushing changes to origin/main"
            if ! git push origin main; then
              echo "• ERROR: Git push failed"
            fi
          else
            echo "• No changes to market data files. Skipping commit."
          fi
        shell: bash

      - name: Upload API data to Download Server via FTP
        run: |
          # Install lftp client (will use cache if available)
          sudo apt-get update && sudo apt-get -y install lftp

          # --- Part 1: Always sync critical v2 data (Protobuf) ---
          echo "Syncing critical v2 data (Protobuf)..."
          lftp -c "
          open -u ${{ secrets.FTP_USERNAME }},${{ secrets.FTP_PASSWORD }} ftp://3117204815.cloudydl.com
          set ssl:verify-certificate no
          # Create parent directory and cd into it for robustness
          mkdir -p public_html/riyales/api/v2
          cd public_html/riyales/api/v2
          # Mirror the local 'market' sub-directory into the current remote directory ('.../api/v2/')
          mirror -R --verbose --delete --parallel=5 api/v2/market/ market
          bye
          "

          # --- Part 2: Conditionally sync recent v1 files (JSON & Assets) ---
          echo "Checking for recent v1 files to sync..."
          LftpV1Commands=""

          # Find recent JSON files and build commands
          RECENT_JSON=$(find api/v1/config -maxdepth 1 -name "*.json" -mmin -60)
          if [ -n "$RECENT_JSON" ]; then
              echo "Found recent JSON files to upload:"
              echo "$RECENT_JSON"
              LftpV1Commands="$LftpV1Commands
              mkdir -p public_html/riyales/api/v1/config
              "
              echo "$RECENT_JSON" | while read -r file; do
                  LftpV1Commands="$LftpV1Commands
                  put -O 'public_html/riyales/api/v1/config' '$file'
                  "
              done
          fi

          # Find recent media assets and build commands
          RECENT_ASSETS=$(find api/v1/config \( -name "*.png" -o -name "*.webp" -o -name "*.svg" -o -name "*.mp4" \) -mmin -60)
          if [ -n "$RECENT_ASSETS" ]; then
              echo "Found recent media assets to upload:"
              echo "$RECENT_ASSETS"
              echo "$RECENT_ASSETS" | while read -r file; do
                  # Example file: "api/v1/config/badges/PWA.webp"
                  RelativePath=$(echo "$file" | sed 's|^api/v1/config/||') # -> "badges/PWA.webp"
                  RemoteDir="public_html/riyales/api/v1/assets/$(dirname "$RelativePath")" # -> "public_html/riyales/api/v1/assets/badges"
                  
                  LftpV1Commands="$LftpV1Commands
                  mkdir -p '$RemoteDir'
                  put -O '$RemoteDir' '$file'
                  "
              done
          fi

          # Execute the combined v1 commands only if there's anything to upload
          if [ -n "$LftpV1Commands" ]; then
              echo "Executing LFTP commands for recent v1 files..."
              lftp -c "
              open -u ${{ secrets.FTP_USERNAME }},${{ secrets.FTP_PASSWORD }} ftp://3117204815.cloudydl.com
              set ssl:verify-certificate no
              $LftpV1Commands
              bye
              "
          else
              echo "No recent v1 files (JSON or Assets) found to upload."
          fi
        shell: bash

      - name: Check and send error logs
        if: always()
        run: |
          ERROR_LOG="${GITHUB_WORKSPACE}/logs/error.log"
          echo "• Final check for error log at: ${ERROR_LOG}"
          
          if [ -f "${ERROR_LOG}" ]; then
            RELEVANT_LINES=$(grep -cvE '(^\s*$)|(^# Log cleared on)|(^# Log forcibly cleared on)' "${ERROR_LOG}" || true)
            
            if [ "${RELEVANT_LINES}" -gt 0 ]; then
              echo "• Error log has relevant content (${RELEVANT_LINES} lines). Running alert_sender.py..."
              cd "${GITHUB_WORKSPACE}"
              python src/alert_sender.py
              
              if [ -s "${ERROR_LOG}" ]; then
                echo "• WARNING: error.log was NOT truncated. Forcing truncation." | tee -a "${ERROR_LOG}"
                echo "# Log forcibly cleared by workflow at $(date -u '+%Y-%m-%dT%H:%M:%SZ')" > "${ERROR_LOG}"
              else
                echo "• error.log successfully truncated."
              fi
            else
              echo "• Error log is empty or contains only clear messages. Skipping alert."
            fi
          else
            echo "• No error.log file found. Skipping alerts."
          fi
        shell: bash

  cleanup:
    name: Cleanup
    needs: sync-data
    if: always()
    runs-on: ubuntu-latest
    permissions:
      actions: write
    env:
      GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      REPO:   ${{ github.repository }}
      WF:     ${{ github.workflow }}
    steps:
      - name: Remove old workflow runs
        run: |
          runs=$(gh run list \
            --repo "$REPO" \
            --workflow "$WF" \
            --limit 100 \
            --json databaseId,status,createdAt \
            --jq '.[] | select(.status!="in_progress" and .status!="queued") | {id: .databaseId, date: .createdAt}' | \
            jq -s 'sort_by(.date) | reverse | .[].id')
          total=$(echo "$runs" | wc -l)
          if [ "$total" -le 9 ]; then exit 0; fi
          to_delete=$(echo "$runs" | tail -n +10)
          echo "$to_delete" | while read id; do
            echo "Deleting run ID: $id"
            gh run delete "$id" --repo "$REPO"
          done
        shell: bash
