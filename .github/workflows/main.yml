name: Market Data Sync

on:
  schedule:
    - cron: '*/5 * * * *'
  workflow_dispatch:
  push:
    branches:
      - main

env:
  PYTHON_VERSION: '3.13'

jobs:
  sync-data:
    name: Sync
    runs-on: ubuntu-latest
    permissions:
      contents: write
    env:
      BRS_API_KEY:        ${{ secrets.BRS_API_KEY }}
      BRS_BASE_URL:       ${{ secrets.BRS_BASE_URL }}
      TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
      TELEGRAM_USER_ID:   ${{ secrets.TELEGRAM_USER_ID }}
      LOG_LEVELS:         ${{ secrets.LOG_LEVELS || 'ERROR' }}
    steps:

      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Restore pip cache
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Cache apt packages for lftp
        uses: actions/cache@v4
        with:
          path: |
            /var/cache/apt/archives
            /var/lib/apt/lists
          key: ${{ runner.os }}-apt-lftp-cache
          restore-keys: |
            ${{ runner.os }}-apt-lftp-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
        shell: bash

      - name: Run data-fetch script
        run: |
          mkdir -p logs
          python src/main.py
        shell: bash

      - name: Pull latest & Commit if any
        run: |
          git config --global user.name "Market Data Bot"
          git config --global user.email "market-data-bot@users.noreply.github.com"
          
          echo "• Pulling latest changes from origin/main"
          if ! git pull --rebase --autostash origin main; then
            echo "• ERROR: Git pull failed with conflicts or errors"
            git rebase --abort 2>/dev/null || true
            git pull origin main || echo "• ERROR: Fallback git pull also failed"
          fi
          
          echo "• Checking for changes in market data files and logs..."
          git add api/v1/
          git add api/v2/market/
          git add logs/
          
          find api/v1/ api/v2/market/ -type f -size +100M | while read file; do
            echo "• Skipping large file from commit: $file"
            git restore --staged "$file"
          done
          
          if ! git diff --staged --quiet; then
            echo "• Changes detected in market data files. Committing..."
            MSG=$(python src/commit_message.py)
            git commit -m "$MSG" -m "Triggered by: ${{ github.event_name }}"
            
            echo "• Pushing changes to origin/main"
            if ! git push origin main; then
              echo "• ERROR: Git push failed"
            fi
          else
            echo "• No changes to market data files. Skipping commit."
          fi
        shell: bash

      # --- CHANGED: Implemented conditional FTP upload based on file modification time ---
      - name: Upload API data to Download Server via FTP
        run: |
          # Install lftp client (will use cache if available)
          sudo apt-get update && sudo apt-get -y install lftp

          # Define directories to check for recent changes
          DIRECTORIES_TO_CHECK="api/v1/config api/v1/banner api/v1/ads api/v1/assets api/v1/terms"

          # Start building the lftp command array. This is safer than a single long string.
          lftp_commands=(
            "open -u ${{ secrets.FTP_USERNAME }},${{ secrets.FTP_PASSWORD }} ftp://3117204815.cloudydl.com"
            "set ssl:verify-certificate no"
            # Part 1: Always sync critical v2 data (Protobuf)
            "mkdir -p public_html/riyales/api/v2"
            "cd public_html/riyales/api/v2"
            "mirror -R --verbose --delete --parallel=5 api/v2/market/ market"
            # Part 2: Navigate to v1 for conditional syncs
            "cd ../v1"
          )

          # Loop through each directory and check for files modified in the last 60 minutes
          for dir_path in $DIRECTORIES_TO_CHECK; do
            # The 'find' command searches for any file (-type f) modified within the last 60 minutes (-mmin -60)
            # If it finds any, the output of the command will not be empty.
            if [ -n "$(find "$dir_path" -type f -mmin -60)" ]; then
              echo "• Recent changes detected in '$dir_path'. Scheduling for upload."
              # Extract the base directory name (e.g., 'config' from 'api/v1/config')
              remote_dir_name=$(basename "$dir_path")
              # Add the mirror command for this directory to our command list
              lftp_commands+=("mirror -R --verbose --delete --parallel=5 '$dir_path' '$remote_dir_name'")
            else
              echo "• No recent changes in '$dir_path'. Skipping upload."
            fi
          done

          # Add the final 'bye' command to close the connection
          lftp_commands+=("bye")

          # Join the array of commands into a single script string separated by semicolons
          lftp_script=$(printf "%s;" "${lftp_commands[@]}")

          # Execute the dynamically generated script
          echo "--- Executing FTP Sync Script ---"
          lftp -c "$lftp_script"
          echo "--- FTP Sync Script Finished ---"
        shell: bash

      - name: Prepare apt directories for caching
        if: always()
        run: |
          sudo chown -R $(whoami):$(whoami) /var/cache/apt/archives /var/lib/apt/lists

      - name: Check and send error logs
        if: always()
        run: |
          ERROR_LOG="${GITHUB_WORKSPACE}/logs/error.log"
          echo "• Final check for error log at: ${ERROR_LOG}"
          
          if [ -f "${ERROR_LOG}" ]; then
            RELEVANT_LINES=$(grep -cvE '(^\s*$)|(^# Log cleared on)|(^# Log forcibly cleared on)' "${ERROR_LOG}" || true)
            
            if [ "${RELEVANT_LINES}" -gt 0 ]; then
              echo "• Error log has relevant content (${RELEVANT_LINES} lines). Running alert_sender.py..."
              cd "${GITHUB_WORKSPACE}"
              python src/alert_sender.py
              
              if [ -s "${ERROR_LOG}" ]; then
                echo "• WARNING: error.log was NOT truncated. Forcing truncation." | tee -a "${ERROR_LOG}"
                echo "# Log forcibly cleared by workflow at $(date -u '+%Y-%m-%dT%H:%M:%SZ')" > "${ERROR_LOG}"
              else
                echo "• error.log successfully truncated."
              fi
            else
              echo "• Error log is empty or contains only clear messages. Skipping alert."
            fi
          else
            echo "• No error.log file found. Skipping alerts."
          fi
        shell: bash

  cleanup:
    name: Cleanup
    needs: sync-data
    if: always()
    runs-on: ubuntu-latest
    permissions:
      actions: write
    env:
      GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      REPO:     ${{ github.repository }}
      WF:       ${{ github.workflow }}
    steps:
      - name: Remove old workflow runs
        run: |
          runs=$(gh run list \
            --repo "$REPO" \
            --workflow "$WF" \
            --limit 100 \
            --json databaseId,status,createdAt \
            --jq '.[] | select(.status!="in_progress" and .status!="queued") | {id: .databaseId, date: .createdAt}' | \
            jq -s 'sort_by(.date) | reverse | .[].id')
          total=$(echo "$runs" | wc -l)
          if [ "$total" -le 9 ]; then exit 0; fi
          to_delete=$(echo "$runs" | tail -n +10)
          echo "$to_delete" | while read id; do
            echo "Deleting run ID: $id"
            gh run delete "$id" --repo "$REPO"
          done
        shell: bash
