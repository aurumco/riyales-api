name: Market Data Sync
on:
  schedule:
    - cron: '*/5 * * * *'
  workflow_dispatch:
  push:
    branches:
      - main
env:
  PYTHON_VERSION: '3.13'
jobs:
  sync-data:
    name: Sync
    runs-on: ubuntu-latest
    permissions:
      contents: write
    env:
      BRS_API_KEY: ${{ secrets.BRS_API_KEY }}
      BRS_BASE_URL: ${{ secrets.BRS_BASE_URL }}
      TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
      TELEGRAM_USER_ID: ${{ secrets.TELEGRAM_USER_ID }}
      LOG_LEVELS: ${{ secrets.LOG_LEVELS || 'ERROR' }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 3

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Restore pip cache
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Cache apt packages for lftp
        uses: actions/cache@v4
        with:
          path: |
            /var/cache/apt/archives
            /var/lib/apt/lists
          key: ${{ runner.os }}-apt-lftp-cache-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-apt-lftp-cache-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
        shell: bash

      - name: Run data-fetch script
        run: |
          mkdir -p logs
          python src/main.py
        shell: bash

      - name: Pull latest & Commit if any (robust)
        id: commit_step
        run: |
          git config --global user.name "Market Data Bot"
          git config --global user.email "market-data-bot@users.noreply.github.com"

          echo "• Fetching origin/main (shallow) to ensure we can compare"
          git fetch --no-tags origin main --depth=3 || git fetch origin main

          # Try to rebase with autostash to preserve generated files, fallback safely.
          echo "• Pulling and rebasing origin/main"
          if ! git pull --rebase --autostash origin main; then
            echo "• WARN: git pull --rebase failed, trying plain pull"
            git rebase --abort 2>/dev/null || true
            git pull origin main || true
          fi

          echo "• Staging generated market data files"
          git add api/v1/ || true
          git add api/v2/market/ || true
          git add logs/ || true

          # Unstage any huge files (>100MB) to avoid accidental commits
          echo "• Checking for very large files to avoid staging them"
          find api/v1/ api/v2/market/ -type f -size +100M | while read file; do
            echo "• Unstaging large file: $file"
            git restore --staged "$file" || true
          done

          # If there are staged changes, commit them
          if ! git diff --staged --quiet; then
            echo "• Changes detected — creating a commit"
            MSG=$(python src/commit_message.py)
            git commit -m "$MSG" -m "Triggered by: ${{ github.event_name }}" || {
              echo "• ERROR: git commit failed"; exit 1;
            }

            # Capture the list of files changed in the last commit (this is authoritative)
            LAST_COMMIT=$(git rev-parse HEAD)
            echo "LAST_COMMIT=$LAST_COMMIT" >> $GITHUB_OUTPUT
            CHANGED_FILES="$(git show --name-only --pretty=\"\" HEAD | sed '/^\s*$/d' || true)"
            echo "• Files in last commit:"
            echo "$CHANGED_FILES" || true

            # Determine which v1 subdirectories changed (write one per line to .changed_dirs)
            TARGET_DIRS="api/v1/config api/v1/banner api/v1/ads api/v1/assets api/v1/terms"
            : > .changed_dirs.tmp
            for d in $TARGET_DIRS; do
              echo "$CHANGED_FILES" | grep -E "^${d}(/|$)" >/dev/null 2>&1 && echo "$d" >> .changed_dirs.tmp
            done

            # If .changed_dirs.tmp is empty but files in api/v1/ changed deeper (e.g. api/v1/assets/badges),
            # detect those too by matching prefix api/v1/assets
            if [ ! -s .changed_dirs.tmp ]; then
              echo "$CHANGED_FILES" | grep -E "^api/v1/" >/dev/null 2>&1 && echo "api/v1" >> .changed_dirs.tmp
            fi

            # Normalize to unique lines and save to workspace file
            sort -u .changed_dirs.tmp > .changed_dirs || true
            echo "• Changed directories list:"
            cat .changed_dirs || echo "(none)"

            # Push changes
            echo "• Pushing commit to origin/main"
            if ! git push origin main; then
              echo "• ERROR: push failed — continuing but uploads may be out-of-sync"
            fi
          else
            echo "• No staged changes. Checking for any unstaged differences (edge cases)."
            # If no commit but there are uncommitted changes (rare), capture them too
            if ! git diff --quiet || ! git status --porcelain | grep -q '^'; then
              echo "• There are uncommitted changes. Staging and committing them now."
              git add api/v1/ api/v2/market/ logs/ || true
              git commit -m "Auto-commit unstaged generated files (workflow)" || true
              LAST_COMMIT=$(git rev-parse HEAD)
              echo "LAST_COMMIT=$LAST_COMMIT" >> $GITHUB_OUTPUT
              git show --name-only --pretty="" HEAD | sed '/^\s*$/d' > .changed_dirs || true
              git push origin main || true
            else
              echo "• Truly nothing changed. Creating empty .changed_dirs for downstream logic."
              : > .changed_dirs
            fi
          fi

          # Ensure .changed_dirs exists (even if empty) so next step can read it
          [ -f .changed_dirs ] || : > .changed_dirs
        shell: bash

      - name: Upload API data to Download Server via FTP (robust detection, uses commit-time list)
        run: |
          # Install lftp
          sudo apt-get update && sudo apt-get -y install lftp

          # Pre-create the main remote directories and prepare lftp commands.
          lftp_commands=(
            "open -u ${{ secrets.FTP_USERNAME }},${{ secrets.FTP_PASSWORD }} ftp://3117204815.cloudydl.com"
            "set ssl:verify-certificate no"
            # Ensure remote base paths exist
            "mkdir -p public_html/riyales/api/v2"
            "mkdir -p public_html/riyales/api/v1"
          )

          # Always mirror v2 (full sync)
          lftp_commands+=("mirror -R --verbose --delete --parallel=5 api/v2/market/ public_html/riyales/api/v2/market")

          # Determine which v1 directories should be uploaded.
          # Prefer authoritative .changed_dirs produced at commit time.
          CHANGED_FILE=".changed_dirs"
          DIRECTORIES_TO_CHECK="api/v1/config api/v1/banner api/v1/ads api/v1/assets api/v1/terms"

          if [ -s "$CHANGED_FILE" ]; then
            echo "• Using commit-time changed_dirs:"
            cat "$CHANGED_FILE"
            while IFS= read -r changed_dir; do
              # If the file contains a general "api/v1" marker, expand to all listed target dirs
              if [ "$changed_dir" = "api/v1" ]; then
                for d in $DIRECTORIES_TO_CHECK; do
                  echo "• Scheduling upload for $d (inferred from generic api/v1 change)"
                  remote_dir_name=$(basename "$d")
                  lftp_commands+=("mirror -R --verbose --delete --parallel=5 '$d' 'public_html/riyales/api/v1/$remote_dir_name'")
                done
                # continue to next line
                continue
              fi

              # Confirm there's actually a file change under that path (safety)
              if git diff --name-only HEAD~1 HEAD -- "$changed_dir" | grep -q '.' 2>/dev/null || true; then
                echo "• Actual content changes detected in '$changed_dir'. Scheduling upload."
                remote_dir_name=$(basename "$changed_dir")
                lftp_commands+=("mirror -R --verbose --delete --parallel=5 '$changed_dir' 'public_html/riyales/api/v1/$remote_dir_name'")
              else
                # fallback: if last commit can't be compared (e.g., HEAD~1 missing), still schedule based on name
                echo "• Scheduling upload for '$changed_dir' (could not verify diff but listed in .changed_dirs)."
                remote_dir_name=$(basename "$changed_dir")
                lftp_commands+=("mirror -R --verbose --delete --parallel=5 '$changed_dir' 'public_html/riyales/api/v1/$remote_dir_name'")
              fi
            done < "$CHANGED_FILE"
          else
            # Fallback: if .changed_dirs is empty, perform direct git diff HEAD~1..HEAD checks (best-effort)
            echo "• No .changed_dirs found (or empty). Falling back to HEAD~1..HEAD diffs."
            PREV_COMMIT=""
            if git rev-parse --verify HEAD~1 >/dev/null 2>&1; then
              PREV_COMMIT=$(git rev-parse HEAD~1)
            fi

            for dir_path in $DIRECTORIES_TO_CHECK; do
              if [ -n "$PREV_COMMIT" ]; then
                if ! git diff --quiet "$PREV_COMMIT" HEAD -- "$dir_path"; then
                  echo "• Actual content changes detected in '$dir_path' (fallback). Scheduling for upload."
                  remote_dir_name=$(basename "$dir_path")
                  lftp_commands+=("mirror -R --verbose --delete --parallel=5 '$dir_path' 'public_html/riyales/api/v1/$remote_dir_name'")
                else
                  echo "• No actual content changes in '$dir_path'. Skipping upload."
                fi
              else
                # If we can't reference PREV_COMMIT, be conservative: check working tree changes
                if git status --porcelain | grep -E "^\\s*[AMRD].*${dir_path}" >/dev/null 2>&1; then
                  echo "• Working tree changes detected in '$dir_path'. Scheduling for upload."
                  remote_dir_name=$(basename "$dir_path")
                  lftp_commands+=("mirror -R --verbose --delete --parallel=5 '$dir_path' 'public_html/riyales/api/v1/$remote_dir_name'")
                else
                  echo "• No detectable changes in '$dir_path'. Skipping upload."
                fi
              fi
            done
          fi

          # finalize and run lftp script
          lftp_commands+=("bye")
          lftp_script=$(printf "%s;" "${lftp_commands[@]}")
          echo "--- Executing FTP Sync Script ---"
          lftp -c "$lftp_script"
          echo "--- FTP Sync Script Finished ---"
        shell: bash

      - name: Check and send error logs
        if: always()
        run: |
          ERROR_LOG="${GITHUB_WORKSPACE}/logs/error.log"
          echo "• Final check for error log at: ${ERROR_LOG}"

          if [ -f "${ERROR_LOG}" ]; then
            RELEVANT_LINES=$(grep -cvE '(^\s*$)|(^# Log cleared on)|(^# Log forcibly cleared on)' "${ERROR_LOG}" || true)

            if [ "${RELEVANT_LINES}" -gt 0 ]; then
              echo "• Error log has relevant content (${RELEVANT_LINES} lines). Running alert_sender.py..."
              cd "${GITHUB_WORKSPACE}"
              python src/alert_sender.py

              if [ -s "${ERROR_LOG}" ]; then
                echo "• WARNING: error.log was NOT truncated. Forcing truncation." | tee -a "${ERROR_LOG}"
                echo "# Log forcibly cleared by workflow at $(date -u '+%Y-%m-%dT%H:%M:%SZ')" > "${ERROR_LOG}"
              else
                echo "• error.log successfully truncated."
              fi
            else
              echo "• Error log is empty or contains only clear messages. Skipping alert."
            fi
          else
            echo "• No error.log file found. Skipping alerts."
          fi
        shell: bash

      - name: Prepare apt directories for caching
        if: always()
        run: |
          sudo chown -R $(whoami):$(whoami) /var/cache/apt/archives /var/lib/apt/lists
        shell: bash

  cleanup:
    name: Cleanup
    needs: sync-data
    if: always()
    runs-on: ubuntu-latest
    permissions:
      actions: write
    env:
      GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      REPO: ${{ github.repository }}
      WF: ${{ github.workflow }}
    steps:
      - name: Remove old workflow runs
        run: |
          runs=$(gh run list \
            --repo "$REPO" \
            --workflow "$WF" \
            --limit 100 \
            --json databaseId,status,createdAt \
            --jq '.[] | select(.status!="in_progress" and .status!="queued") | {id: .databaseId, date: .createdAt}' | \
            jq -s 'sort_by(.date) | reverse | .[].id')
          total=$(echo "$runs" | wc -l)
          if [ "$total" -le 9 ]; then exit 0; fi
          to_delete=$(echo "$runs" | tail -n +10)
          echo "$to_delete" | while read id; do
            echo "Deleting run ID: $id"
            gh run delete "$id" --repo "$REPO"
          done
        shell: bash
